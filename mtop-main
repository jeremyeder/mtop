#!/usr/bin/env python3
"""mtop - Mock CLI tool for debugging LLMInferenceService CRDs"""

import sys

# Check Python version early
if sys.version_info < (3, 11):
    print("Error: mtop requires Python 3.11 or later")
    print(f"Current version: {sys.version}")
    sys.exit(1)

import argparse
import json
import os
import random
import signal
import subprocess
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import yaml
from termcolor import colored

# Try to import SLO dashboard components
try:
    from config_loader import SLOConfig, TechnologyConfig, WorkloadConfig
    from mtop.slo_convergence import SLOConvergenceAlgorithm
    from mtop.slo_dashboard import SLODashboard

    SLO_DASHBOARD_AVAILABLE = True
except ImportError as e:
    SLO_DASHBOARD_AVAILABLE = False
    SLO_IMPORT_ERROR = str(e)

# Try to import rich for ldtop functionality
try:
    from rich.console import Console
    from rich.layout import Layout
    from rich.live import Live
    from rich.panel import Panel
    from rich.table import Table
    from rich.text import Text

    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False


class ModelMetrics:
    """Represents metrics for a single model - extracted from demo_monitor.py"""

    def __init__(self, name: str, cr_data: Dict[str, Any], base_qps: int = 100):
        self.name = name
        self.cr_data = cr_data
        self.base_qps = base_qps
        self.current_qps = base_qps + random.randint(-20, 20)
        self.target_qps = base_qps
        self.cpu_percent = random.randint(20, 60)
        self.memory_percent = random.randint(30, 50)
        self.error_rate = random.uniform(0.1, 2.0)
        self.latency_p95 = random.randint(80, 200)
        self.replicas = random.randint(1, 5)
        self.traffic_percent = 100

        # Derive status from CR data
        ready_condition = next(
            (c for c in cr_data.get("status", {}).get("conditions", []) if c["type"] == "Ready"), {}
        )
        self.status = "Ready" if ready_condition.get("status") == "True" else "NotReady"

    def update(self, action: str = "normal"):
        """Update metrics with some randomness"""
        # Add some realistic variation
        self.current_qps += random.uniform(-15, 15)
        self.current_qps = max(10, self.current_qps)

        self.cpu_percent += random.randint(-5, 5)
        self.cpu_percent = max(10, min(95, self.cpu_percent))

        self.error_rate += random.uniform(-0.2, 0.2)
        self.error_rate = max(0.1, min(10.0, self.error_rate))

        self.latency_p95 += random.randint(-10, 10)
        self.latency_p95 = max(50, min(500, self.latency_p95))


class LLMTopMonitor:
    """mtop monitoring functionality"""

    def __init__(self, mtop: "MTop", interval: float = 0.5, namespace: str = "default"):
        if not RICH_AVAILABLE:
            raise ImportError("Rich library is required for ldtop. Install with: pip install rich")

        self.mtop = mtop
        self.console = Console()
        self.interval = interval
        self.namespace = namespace
        self.running = True
        self.sort_key = "qps"  # qps, cpu, errors, name
        self.models: Dict[str, ModelMetrics] = {}
        self.start_time = time.time()

        # Setup signal handlers
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _signal_handler(self, signum, frame):
        """Handle signals for clean exit"""
        self.running = False

    def _load_models(self):
        """Load models from mock data or live cluster"""
        self.models = {}

        if self.mtop.is_live:
            self._load_live_models()
        else:
            self._load_mock_models()

    def _load_live_models(self):
        """Load models from live Kubernetes cluster"""
        try:
            # Get LLMInferenceService resources from the cluster
            data = self.mtop.k8s_get("llminferenceservice", namespace=self.namespace)
            if "items" in data:
                for item in data["items"]:
                    name = item["metadata"]["name"]
                    base_qps = random.randint(50, 800)  # Vary base QPS by model
                    self.models[name] = ModelMetrics(name, item, base_qps)
            else:
                # Single item response
                name = data["metadata"]["name"]
                base_qps = random.randint(50, 800)
                self.models[name] = ModelMetrics(name, data, base_qps)
        except Exception as e:
            self.console.print(f"[red]Error loading live models: {e}[/red]")
            self.console.print("[yellow]Falling back to mock mode[/yellow]")
            self._load_mock_models()

    def _load_mock_models(self):
        """Load models from mock CRs"""
        for cr_file in sorted(self.mtop.crs_dir.glob("*.json")):
            try:
                with open(cr_file) as f:
                    cr_data = json.load(f)
                name = cr_data["metadata"]["name"]
                base_qps = random.randint(50, 800)  # Vary base QPS by model
                self.models[name] = ModelMetrics(name, cr_data, base_qps)
            except Exception as e:
                self.console.print(f"[red]Error loading {cr_file}: {e}[/red]")

    def _create_traffic_table(self) -> Table:
        """Create the main traffic monitoring table"""
        table = Table(show_header=True, header_style="bold cyan")
        table.add_column("Model", width=30)
        table.add_column("Status", width=12, justify="center")
        table.add_column("QPS", width=8, justify="right")
        table.add_column("GPU %util", width=9, justify="center")
        table.add_column("Errors", width=7, justify="center")
        table.add_column("Latency", width=8, justify="center")
        table.add_column("Replicas", width=8, justify="center")

        # Sort models based on current sort key
        if self.sort_key == "qps":
            sorted_models = sorted(
                self.models.items(), key=lambda x: x[1].current_qps, reverse=True
            )
        elif self.sort_key == "cpu":
            sorted_models = sorted(
                self.models.items(), key=lambda x: x[1].cpu_percent, reverse=True
            )
        elif self.sort_key == "errors":
            sorted_models = sorted(self.models.items(), key=lambda x: x[1].error_rate, reverse=True)
        else:  # name
            sorted_models = sorted(self.models.items(), key=lambda x: x[0])

        for name, metrics in sorted_models:
            # Status indicator
            if metrics.status == "Ready":
                status_emoji = "ðŸŸ¢"
            else:
                status_emoji = "ðŸ”´"

            # Error rate coloring
            error_style = (
                "green" if metrics.error_rate < 1 else "yellow" if metrics.error_rate < 3 else "red"
            )

            # CPU coloring
            cpu_style = (
                "green"
                if metrics.cpu_percent < 50
                else "yellow"
                if metrics.cpu_percent < 80
                else "red"
            )

            table.add_row(
                name[:23] + ("..." if len(name) > 23 else ""),
                f"{status_emoji} {metrics.status[:6]}",
                f"{int(metrics.current_qps):,}",
                f"[{cpu_style}]{metrics.cpu_percent}%[/{cpu_style}]",
                f"[{error_style}]{metrics.error_rate:.1f}%[/{error_style}]",
                f"{metrics.latency_p95}ms",
                str(metrics.replicas),
            )

        return table

    def _create_summary_panel(self) -> Panel:
        """Create cluster summary panel"""
        if not self.models:
            return Panel("No models loaded", border_style="red")

        total_qps = sum(m.current_qps for m in self.models.values())
        total_replicas = sum(m.replicas for m in self.models.values())
        avg_cpu = sum(m.cpu_percent for m in self.models.values()) / len(self.models)
        avg_error = sum(m.error_rate for m in self.models.values()) / len(self.models)

        healthy_models = sum(1 for m in self.models.values() if m.status == "Ready")
        total_models = len(self.models)

        runtime = time.time() - self.start_time

        summary_text = Text()
        summary_text.append(f"ðŸš€ mtop - LLM Inference Monitor\n", style="bold cyan")
        summary_text.append(
            f"Mode: {self.mtop.mode} | Namespace: {self.namespace} | Runtime: {runtime:.0f}s | Sort: {self.sort_key}\n",
            style="dim",
        )
        summary_text.append(f"Total QPS: {int(total_qps):,} ", style="white")
        summary_text.append(
            f"â€¢ Models: {healthy_models}/{total_models} healthy ",
            style="green" if healthy_models == total_models else "yellow",
        )
        summary_text.append(f"â€¢ Replicas: {total_replicas}\n", style="white")
        summary_text.append(f"Controls: Ctrl+C to quit | Sort by: {self.sort_key}", style="dim")

        return Panel(summary_text, border_style="cyan")

    def _create_layout(self) -> Layout:
        """Create the monitor layout"""
        layout = Layout()
        layout.split_column(Layout(name="summary", size=6), Layout(name="main"))

        layout["summary"].update(self._create_summary_panel())
        layout["main"].update(
            Panel(
                self._create_traffic_table(),
                title="ðŸ”¥ Live LLM Inference Traffic",
                border_style="green",
            )
        )

        return layout

    def _handle_input(self):
        """Handle keyboard input (simplified - in real implementation would use proper input handling)"""
        # This is a simplified version - real implementation would need proper non-blocking input
        pass

    def run(self):
        """Run the ldtop monitor"""
        self.console.print("[cyan]Starting mtop monitor...[/cyan]")
        self.console.print("[dim]Press Ctrl+C to exit[/dim]")

        # Initial load
        self._load_models()

        if not self.models:
            self.console.print("[red]No models found to monitor[/red]")
            return

        try:
            with Live(self._create_layout(), refresh_per_second=1 / self.interval) as live:
                while self.running:
                    # Update all models with some variation
                    for model in self.models.values():
                        model.update()

                    # Update display
                    live.update(self._create_layout())

                    time.sleep(self.interval)

        except KeyboardInterrupt:
            self.console.print("\n[yellow]Exiting mtop...[/yellow]")
        except Exception as e:
            self.console.print(f"[red]Error: {e}[/red]")


class MTop:
    def __init__(self, mode: str = "mock") -> None:
        self.mode = mode
        self.is_live = mode == "live"
        self.mock_root = Path(__file__).parent / "mocks"
        self.crs_dir = self.mock_root / "crs"
        self.config_path = self.mock_root / "config" / "llminferenceserviceconfig.json"
        self.logs_dir = self.mock_root / "pod_logs"
        self.states_dir = self.mock_root / "states" / "rollout"

    def k8s_get(
        self, resource: str, name: Optional[str] = None, namespace: str = "default"
    ) -> Dict[str, Any]:
        """Execute kubectl get command and return JSON"""
        cmd = ["kubectl", "get", resource]
        if name:
            cmd.append(name)
        cmd.extend(["-n", namespace, "-o", "json"])

        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            print(colored(f"Error: {result.stderr.strip()}", "red"))
            sys.exit(1)
        return json.loads(result.stdout)

    def list_crs(self) -> None:
        """List all LLMInferenceService resources"""
        if self.is_live:
            data = self.k8s_get("llminferenceservice")
            for item in data.get("items", []):
                name = item["metadata"]["name"]
                ready = next(
                    (
                        c
                        for c in item.get("status", {}).get("conditions", [])
                        if c["type"] == "Ready"
                    ),
                    {},
                )
                status = ready.get("status", "Unknown")
                color = "green" if status == "True" else "red"
                print(f"{name:35} | Ready: " + colored(status, color))
        else:
            for cr_file in sorted(self.crs_dir.glob("*.json")):
                with open(cr_file) as f:
                    data = json.load(f)
                name = data["metadata"]["name"]
                ready = next(
                    (
                        c
                        for c in data.get("status", {}).get("conditions", [])
                        if c["type"] == "Ready"
                    ),
                    {},
                )
                status = ready.get("status", "Unknown")
                color = "green" if status == "True" else "red"
                print(f"{name:35} | Ready: " + colored(status, color))

    def get_cr(self, name: str, output_json: bool = False) -> None:
        """Get a specific CR by name"""
        if self.is_live:
            data = self.k8s_get("llminferenceservice", name)
        else:
            cr_path = self.crs_dir / f"{name}.json"
            if not cr_path.exists():
                print(colored(f"âŒ {name} not found.", "red"))
                return
            with open(cr_path) as f:
                data = json.load(f)

        if output_json:
            print(json.dumps(data, indent=2))
        else:
            print(yaml.safe_dump(data, sort_keys=False))

    def get_config(self, output_json: bool = False) -> None:
        """Show LLMInferenceServiceConfig"""
        if self.is_live:
            data = self.k8s_get("llminferenceserviceconfig")
        else:
            if not self.config_path.exists():
                print(colored(f"âš ï¸ No config found at {self.config_path}", "yellow"))
                return
            with open(self.config_path) as f:
                data = json.load(f)

        if output_json:
            print(json.dumps(data, indent=2))
        else:
            print(yaml.safe_dump(data, sort_keys=False))

    def check_cr(self, name: str) -> None:
        """Check status of a CR"""
        if self.is_live:
            data = self.k8s_get("llminferenceservice", name)
        else:
            cr_path = self.crs_dir / f"{name}.json"
            if not cr_path.exists():
                print(colored(f"{name} not found.", "red"))
                return
            with open(cr_path) as f:
                data = json.load(f)

        print(f"ðŸ” {name} status:")
        for cond in data.get("status", {}).get("conditions", []):
            print(f" - [{cond['type']}] Status: {cond['status']} - {cond.get('message', '')}")

    def show_logs(self, name: str) -> None:
        """Show logs for a service"""
        if self.is_live:
            cmd = ["kubectl", "logs", f"deployment/{name}", "-n", "default"]
            result = subprocess.run(cmd, capture_output=True, text=True)
            print(result.stdout if result.returncode == 0 else result.stderr)
        else:
            log_path = self.logs_dir / f"{name}.txt"
            if log_path.exists():
                print(log_path.read_text())
            else:
                print(f"No logs found for {name}.")

    def delete_cr(self, name: str) -> None:
        """Delete a CR"""
        if self.is_live:
            cmd = ["kubectl", "delete", "llminferenceservice", name, "-n", "default"]
            result = subprocess.run(cmd, capture_output=True, text=True)
            print(result.stdout if result.returncode == 0 else result.stderr)
        else:
            cr_path = self.crs_dir / f"{name}.json"
            if cr_path.exists():
                cr_path.unlink()
                print(colored(f"âœ… {name} deleted.", "green"))
            else:
                print(colored(f"{name} not found.", "red"))

    def create_cr(self, file_path: Union[str, Path]) -> None:
        """Create a CR from file"""
        file_path = Path(file_path)
        if not file_path.exists():
            print(colored(f"âŒ File not found: {file_path}", "red"))
            return

        try:
            with open(file_path) as f:
                if file_path.suffix == ".yaml":
                    data = yaml.safe_load(f)
                else:
                    data = json.load(f)
        except (json.JSONDecodeError, yaml.YAMLError) as e:
            print(colored(f"âŒ Error parsing {file_path.suffix} file: {e}", "red"))
            return
        except Exception as e:
            print(colored(f"âŒ Error reading file {file_path}: {e}", "red"))
            return

        try:
            if self.is_live:
                cmd = ["kubectl", "apply", "-f", str(file_path)]
                result = subprocess.run(cmd, capture_output=True, text=True)
                print(result.stdout if result.returncode == 0 else result.stderr)
            else:
                if (
                    not isinstance(data, dict)
                    or "metadata" not in data
                    or "name" not in data["metadata"]
                ):
                    print(f"âŒ Invalid CR format: missing metadata.name field")
                    return

                name = data["metadata"]["name"]
                cr_path = self.crs_dir / f"{name}.json"
                self.crs_dir.mkdir(parents=True, exist_ok=True)

                with open(cr_path, "w") as f:
                    json.dump(data, f, indent=2)
                print(colored(f"âœ… Created {name}", "green"))
        except Exception as e:
            print(f"âŒ Error creating CR: {e}")
            return

    def simulate_rollout(self, topology: str) -> None:
        """Simulate a rollout"""
        topology_dir = self.states_dir / topology
        if not topology_dir.exists():
            print(colored(f"âŒ No rollout steps found for topology: {topology}", "red"))
            return

        steps = sorted(topology_dir.glob("step*.json"))
        print(colored(f"ðŸ“¡ Simulating rollout for '{topology}' with {len(steps)} steps...", "blue"))

        for step_path in steps:
            with open(step_path) as f:
                step = json.load(f)
            print(f"Step {step['step']} @ {step['timestamp']}:")
            for model, status in step["status"].items():
                traffic = step["traffic"].get(model, 0)
                print(f"  - {model:25s} {status['status']:12s} {traffic:>3}%")

        print(colored("âœ… Simulation complete.", "green"))

    def list_topologies(self) -> None:
        """List available rollout topologies"""
        if not self.states_dir.exists():
            print("âŒ No rollout topologies found.")
            return

        print("ðŸ“¦ Available rollout topologies:")
        for d in sorted(self.states_dir.iterdir()):
            if d.is_dir():
                print(f"  - {d.name}")

    def ldtop(self, interval: float = 0.5, namespace: str = "default", **kwargs) -> None:
        """Run mtop monitoring"""
        if not RICH_AVAILABLE:
            print(
                colored(
                    "âŒ Rich library is required for ldtop. Install with: pip install rich", "red"
                )
            )
            sys.exit(1)

        monitor = LLMTopMonitor(self, interval=interval, namespace=namespace)
        monitor.run()

    def slo_dashboard(
        self, interval: float = 1.0, demo: bool = False, models: Optional[List[str]] = None
    ) -> None:
        """Run SLO dashboard with convergence monitoring"""
        if not SLO_DASHBOARD_AVAILABLE:
            print(
                colored(f"âŒ SLO dashboard components are not available: {SLO_IMPORT_ERROR}", "red")
            )
            sys.exit(1)

        if not RICH_AVAILABLE:
            print(
                colored(
                    "âŒ Rich library is required for SLO dashboard. Install with: pip install rich",
                    "red",
                )
            )
            sys.exit(1)

        console = Console()
        console.print("[cyan]ðŸš€ Starting SLO Dashboard...[/cyan]")
        console.print("[dim]Press Ctrl+C to exit[/dim]\n")

        try:
            if demo:
                # Use demo mode from slo_dashboard.py
                from mtop.slo_dashboard import demo_dashboard

                demo_dashboard()
            else:
                # Live integration mode
                console.print("[yellow]Live integration mode not yet implemented[/yellow]")
                console.print("[dim]Use --demo flag for demonstration mode[/dim]")

                # Create basic SLO config for live mode
                slo_config = SLOConfig(
                    ttft_p95_ms=250.0,
                    tokens_per_second=1000.0,
                    error_rate_percent=1.0,
                )

                dashboard = SLODashboard(slo_config, console)

                # Basic dashboard display without live data
                console.print("[bold blue]SLO Dashboard - Live Mode (Coming Soon)[/bold blue]")
                console.print("For now, please use: mtop slo-dashboard --demo")

        except KeyboardInterrupt:
            console.print("\n[yellow]SLO Dashboard stopped[/yellow]")
        except Exception as e:
            console.print(f"[red]Error running SLO dashboard: {e}[/red]")


def main() -> None:
    # Check if running as mtop-monitor
    script_name = Path(sys.argv[0]).name
    if script_name == "mtop-monitor":
        # When called as mtop-monitor, insert ldtop command
        if len(sys.argv) == 1:
            # No arguments, just run ldtop
            sys.argv.append("ldtop")
        else:
            # Arguments provided, insert ldtop as the command
            sys.argv.insert(1, "ldtop")

    parser = argparse.ArgumentParser(
        prog="mtop",
        description="ðŸš€ mtop: Mock CLI tool for debugging LLMInferenceService CRDs",
        epilog="""
Examples:
  %(prog)s list                    # List all LLMInferenceServices  
  %(prog)s get gpt2               # Get specific CR in YAML format
  %(prog)s --json get gpt2        # Get specific CR in JSON format
  %(prog)s simulate canary        # Simulate canary rollout
  %(prog)s --mode live list       # Use live Kubernetes cluster
  %(prog)s create model.yaml      # Create CR from file
  %(prog)s ldtop                  # Real-time LLM inference monitoring
  %(prog)s --mode live ldtop      # Monitor live cluster
  %(prog)s slo-dashboard          # SLO dashboard with convergence monitoring
  %(prog)s slo-dashboard --demo   # SLO dashboard in demo mode
  
For more information: https://github.com/jeremyeder/mtop
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--mode",
        choices=["mock", "live"],
        help="Execution mode: 'mock' uses local files, 'live' uses kubectl",
    )
    parser.add_argument("--json", action="store_true", help="Output in JSON format (default: YAML)")
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Enable verbose output for debugging"
    )

    subparsers = parser.add_subparsers(
        dest="command", help="Available commands", title="commands", metavar="<command>"
    )

    # Simple commands
    subparsers.add_parser("list", help="List all LLMInferenceServices with their status")
    subparsers.add_parser("config", help="Display the global LLM configuration")
    subparsers.add_parser("list-topologies", help="List available rollout topology types")
    subparsers.add_parser("help", help="Show detailed help information")

    # ldtop command
    ldtop_parser = subparsers.add_parser(
        "ldtop", help="Real-time LLM inference monitoring (like htop for LLMs)"
    )
    ldtop_parser.add_argument(
        "--interval", type=float, default=0.5, help="Refresh interval in seconds (default: 0.5)"
    )
    ldtop_parser.add_argument(
        "--namespace", default="default", help="Kubernetes namespace to monitor"
    )

    # slo-dashboard command
    slo_dashboard_parser = subparsers.add_parser(
        "slo-dashboard", help="Real-time SLO dashboard with convergence monitoring"
    )
    slo_dashboard_parser.add_argument(
        "--interval", type=float, default=1.0, help="Refresh interval in seconds (default: 1.0)"
    )
    slo_dashboard_parser.add_argument(
        "--demo", action="store_true", help="Run in demo mode with simulated data"
    )
    slo_dashboard_parser.add_argument(
        "--models", nargs="*", help="Specific models to monitor (default: all)"
    )

    # Commands with arguments
    get_parser = subparsers.add_parser(
        "get", help="Retrieve and display a specific LLMInferenceService"
    )
    get_parser.add_argument("name", help="CR name")

    check_parser = subparsers.add_parser(
        "check", help="Check the detailed status of a specific service"
    )
    check_parser.add_argument("name", help="CR name")

    logs_parser = subparsers.add_parser("logs", help="Display logs for a specific service")
    logs_parser.add_argument("name", help="Service name")

    delete_parser = subparsers.add_parser("delete", help="Delete a specific LLMInferenceService")
    delete_parser.add_argument("name", help="CR name")

    create_parser = subparsers.add_parser(
        "create", help="Create a new LLMInferenceService from file"
    )
    create_parser.add_argument("file", help="JSON/YAML file path")

    simulate_parser = subparsers.add_parser(
        "simulate", help="Simulate a deployment rollout with specified topology"
    )
    simulate_parser.add_argument("topology", help="Topology name")

    args = parser.parse_args()

    # Determine mode
    mode = args.mode or os.environ.get("LLD_MODE", "mock")
    # Don't show mode output when using --json flag for get/config commands
    show_mode = mode == "mock" and not (hasattr(args, "json") and args.json)
    if show_mode:
        print(colored(f"ðŸ”§ Mode: {mode}", "cyan"))

    cli = MTop(mode=mode)

    # Command dispatch
    commands = {
        "list": cli.list_crs,
        "config": lambda: cli.get_config(args.json),
        "get": lambda: cli.get_cr(args.name, args.json),
        "check": lambda: cli.check_cr(args.name),
        "logs": lambda: cli.show_logs(args.name),
        "delete": lambda: cli.delete_cr(args.name),
        "create": lambda: cli.create_cr(args.file),
        "simulate": lambda: cli.simulate_rollout(args.topology),
        "list-topologies": cli.list_topologies,
        "ldtop": lambda: cli.ldtop(interval=args.interval, namespace=args.namespace),
        "slo-dashboard": lambda: cli.slo_dashboard(
            interval=args.interval, demo=args.demo, models=args.models
        ),
        "help": parser.print_help,
        None: parser.print_help,
    }

    command_func = commands.get(args.command, parser.print_help)
    command_func()


if __name__ == "__main__":
    main()
