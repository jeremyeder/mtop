# ${PROGRAM_NAME}

![CI](https://github.com/jeremyeder/${PROGRAM_NAME}/workflows/CI/badge.svg)
![Tests](https://img.shields.io/badge/tests-53%20passing-green.svg)
![Coverage](https://img.shields.io/badge/coverage-comprehensive-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![Quality](https://img.shields.io/badge/code%20quality-production%20ready-green.svg)

`${PROGRAM_NAME}` (LLM Debug) is a mock CLI tool for debugging and simulating `LLMInferenceService` CRDs in a Kubernetes-like environment â€” without requiring a live cluster. It's designed for SREs, ML engineers, and platform teams to simulate rollouts, test failure modes, and inspect the status of LLM-serving infrastructure offline.

> **Note**: This project was initially generated by ChatGPT but completely rewritten by Claude due to severe structural issues in the original code.

---

## ${EMOJI} Features

- âœ… Offline simulation of up to 100 `LLMInferenceService` CRs
- âœ… Mock support for `LLMInferenceServiceConfig`
- âœ… Full CRUD support on mocked CRs
- âœ… Realistic rollout playback (`play rollout`)
- âœ… Live-style watch command (`watch`) to track CR status changes
- âœ… Log simulation for individual model services
- âœ… Configurable via file structure (`./mocks/`)
- âœ… **${PROGRAM_NAME}top**: Real-time LLM inference monitoring (like `htop` for LLMs)

---

## ğŸ“¦ Structure

```
${PROGRAM_NAME}/
â”œâ”€â”€ ${PROGRAM_NAME}                 # Python CLI entry point
â”œâ”€â”€ mocks/
â”‚   â”œâ”€â”€ crs/                   # LLMInferenceService CRs (20 included)
â”‚   â”œâ”€â”€ config/                # Global LLMInferenceServiceConfig mock
â”‚   â”œâ”€â”€ pod_logs/              # Simulated logs for LLMs
â”‚   â””â”€â”€ topologies/            # Rollout scenarios
```

---

## ğŸ›  Usage

```bash
chmod +x ${PROGRAM_NAME}
./${PROGRAM_NAME} list                         # List all mocked LLMInferenceServices
./${PROGRAM_NAME} check llm-005                # Inspect a specific CR
./${PROGRAM_NAME} logs llm-005                 # View logs for a model
./${PROGRAM_NAME} config                       # Show global LLM config
./${PROGRAM_NAME} delete llm-005               # Delete a mock CR
./${PROGRAM_NAME} create file.json             # Create CR from file
./${PROGRAM_NAME} simulate canary              # Simulate canary rollout
./${PROGRAM_NAME} ${MONITOR_NAME}                        # Real-time LLM inference monitoring
```

---

## ğŸ”¥ ${PROGRAM_NAME}top: Real-time LLM Monitoring

`${PROGRAM_NAME}top` provides real-time monitoring of LLM inference services, similar to `htop` but specifically designed for LLM workloads.

### Features
- **Real-time metrics**: QPS, CPU usage, error rates, latency
- **Multi-model monitoring**: Track multiple LLM services simultaneously  
- **Dual-mode operation**: Works with both mock data and live clusters
- **Interactive display**: Live-updating terminal interface
- **Cluster overview**: Aggregate statistics and health indicators

### Usage

```bash
# Monitor using mock data (default)
./${PROGRAM_NAME} ${MONITOR_NAME}

# Monitor live cluster
./${PROGRAM_NAME} --mode live ${MONITOR_NAME}

# Monitor specific namespace
./${PROGRAM_NAME} ${MONITOR_NAME} --namespace production

# Custom refresh rate
./${PROGRAM_NAME} ${MONITOR_NAME} --interval 1.0

# As kubectl plugin
./${PROGRAM_NAME}top                  # Via symlink
```

### Display

```
${EMOJI} ${PROGRAM_NAME}top - LLM Inference Monitor
Mode: mock | Namespace: default | Runtime: 45s | Sort: qps
Total QPS: 12,847 â€¢ Models: 34/34 healthy â€¢ Replicas: 127
Avg CPU: 52.3% â€¢ Avg Errors: 0.8%
Controls: Ctrl+C to quit | Sort by: qps

â”Œâ”€â”€â”€ ğŸ”¥ Live LLM Inference Traffic â”€â”€â”€â”
â”‚ Model                  Status    QPS   CPU   Errors  Latency  Replicas â”‚
â”‚ llama-3-70b-instruct   ğŸŸ¢ Ready   2,847  45%   0.2%    127ms    8       â”‚
â”‚ gpt-4-turbo           ğŸŸ¢ Ready   2,234  62%   0.1%    98ms     6       â”‚
â”‚ claude-3-opus         ğŸŸ¢ Ready   1,923  58%   0.3%    145ms    5       â”‚
â”‚ mixtral-8x7b-instruct ğŸŸ¢ Ready   1,445  71%   0.5%    156ms    4       â”‚
â”‚ ...                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Requirements

${PROGRAM_NAME}top requires the `rich` library for terminal display:

```bash
pip install rich
```

---

## ğŸ¬ Rollout Simulation

```bash
./${PROGRAM_NAME} simulate canary
./${PROGRAM_NAME} simulate bluegreen
./${PROGRAM_NAME} simulate rolling
./${PROGRAM_NAME} simulate shadow
```

Watch rollouts with animation:

```bash
./watch_rollout.py --topology rolling --autoplay --delay 2
```

---

## ğŸŒ Live Cluster Support

`${PROGRAM_NAME}` can operate in **live mode** using actual Kubernetes resources.

### ğŸ” Modes

| Mode   | Description                       |
|--------|-----------------------------------|
| `mock` | Use local files in `./mocks/`     |
| `live` | Use `kubectl` to talk to a cluster|

### ğŸ”§ Set the Mode

Via environment variable:

```bash
export LLD_MODE=live
./${PROGRAM_NAME} list
```

Or per-command:

```bash
./${PROGRAM_NAME} --mode live list
./${PROGRAM_NAME} --mode mock list
```

---

## ğŸ§ª Development & Testing

**Requires: Python 3.11+**

The tool will check your Python version on startup and exit with an error if you're running an older version.

### Code Quality Features
- âœ… **Comprehensive Testing**: 53 tests including unit and integration tests
- âœ… **Type Hints**: Full type annotations for better IDE support
- âœ… **Security Scanning**: Automated vulnerability scanning with safety and bandit
- âœ… **Code Quality Gates**: Coverage thresholds and complexity checks
- âœ… **Dependency Management**: Automated updates with Dependabot
- âœ… **Error Handling**: Graceful error handling for all operations

### Running Tests
```bash
# Install test dependencies
pip install -r requirements.txt

# Run all tests
python3 -m pytest tests/ -v

# Run with coverage
python3 -m pytest tests/ --cov=. --cov-report=term
```

### Dependencies
See `requirements.txt` for pinned dependency versions with security scanning.

---

## Installation

### 1. Install Python dependencies

**Requirements: Python 3.11 or later**

```bash
# Check your Python version
python3 --version

# Install dependencies
pip install -r requirements.txt
```

### 2. Make executable

```bash
chmod +x ${PROGRAM_NAME}
```

### 3. Test installation

```bash
./${PROGRAM_NAME} help
./${PROGRAM_NAME} list
```

---

## ğŸ“œ License

MIT

---

## ğŸ¤ Contributing

Pull requests welcome for:
- Additional rollout topologies
- New simulation features
- Improved error handling
- Enhanced testing